{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6192ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/drey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/drey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b5026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import annotated dataset\n",
    "df = pd.read_csv('/Users/drey/Downloads/User Experience Spectrum Task - Annotated_data (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ac798d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Randomizer</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>User Experience Spectrum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A1NAA1R38JSNHV</td>\n",
       "      <td>B0002F7IIK</td>\n",
       "      <td>Josh Leger</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>1/4/2013</td>\n",
       "      <td>works wonders, very durable</td>\n",
       "      <td>Will hold any guitar that I've seen. It holds ...</td>\n",
       "      <td>Thrilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADH0O8UVJOT10</td>\n",
       "      <td>B007Q27BH0</td>\n",
       "      <td>StormJH1</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>5</td>\n",
       "      <td>12/31/2012</td>\n",
       "      <td>Another instant classic from Joyo</td>\n",
       "      <td>The US Dream is supposed \"clone\" of the Suhr R...</td>\n",
       "      <td>Insightful Feedback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>A2E3Q52SJS00K2</td>\n",
       "      <td>B004N0MKN8</td>\n",
       "      <td>Wendell Burnett</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>1/3/2014</td>\n",
       "      <td>Great Idea</td>\n",
       "      <td>Those of us who have leaned their guitars agai...</td>\n",
       "      <td>Thrilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>A4BTCECGQAIUI</td>\n",
       "      <td>B0002M6CVC</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>5/30/2013</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good Strings, I buy this Strings from a few ye...</td>\n",
       "      <td>Thrilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>A7IBOCJ0K4V8C</td>\n",
       "      <td>B003VWJ2K8</td>\n",
       "      <td>J. Walker</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>5/6/2013</td>\n",
       "      <td>It works.</td>\n",
       "      <td>This tuner can be adjusted to be very easy to ...</td>\n",
       "      <td>Insightful Feedback</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Randomizer      reviewerID        asin     reviewerName helpful  overall  \\\n",
       "0           1  A1NAA1R38JSNHV  B0002F7IIK       Josh Leger  [0, 0]        5   \n",
       "1           2   ADH0O8UVJOT10  B007Q27BH0         StormJH1  [3, 3]        5   \n",
       "2           4  A2E3Q52SJS00K2  B004N0MKN8  Wendell Burnett  [0, 0]        5   \n",
       "3           6   A4BTCECGQAIUI  B0002M6CVC  Amazon Customer  [0, 2]        4   \n",
       "4           7   A7IBOCJ0K4V8C  B003VWJ2K8        J. Walker  [0, 0]        5   \n",
       "\n",
       "   reviewTime                            summary  \\\n",
       "0    1/4/2013        works wonders, very durable   \n",
       "1  12/31/2012  Another instant classic from Joyo   \n",
       "2    1/3/2014                         Great Idea   \n",
       "3   5/30/2013                               Good   \n",
       "4    5/6/2013                          It works.   \n",
       "\n",
       "                                          reviewText User Experience Spectrum  \n",
       "0  Will hold any guitar that I've seen. It holds ...                 Thrilled  \n",
       "1  The US Dream is supposed \"clone\" of the Suhr R...      Insightful Feedback  \n",
       "2  Those of us who have leaned their guitars agai...                 Thrilled  \n",
       "3  Good Strings, I buy this Strings from a few ye...                 Thrilled  \n",
       "4  This tuner can be adjusted to be very easy to ...      Insightful Feedback  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69f404",
   "metadata": {},
   "source": [
    "### EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfcc9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Randomizer'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e55de0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>User Experience Spectrum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>A22Z554ZQ8NFPC</td>\n",
       "      <td>B0002D0CKI</td>\n",
       "      <td>AF \"Whigs\"</td>\n",
       "      <td>[12, 12]</td>\n",
       "      <td>3</td>\n",
       "      <td>9/8/2011</td>\n",
       "      <td>Good &amp; Bad</td>\n",
       "      <td>First, I love these picks and used them exclus...</td>\n",
       "      <td>Mixed Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>A1B9Q3SNKI6T5V</td>\n",
       "      <td>B0002D0LKY</td>\n",
       "      <td>Dustin Kempton</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>5/2/2014</td>\n",
       "      <td>I really do hate it.</td>\n",
       "      <td>It just randomly pops off my bass, it's so sli...</td>\n",
       "      <td>Opportunity Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>A3NAA6BH9LWIH4</td>\n",
       "      <td>B001UJEKZ6</td>\n",
       "      <td>Paul Kacprzak</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>5/15/2013</td>\n",
       "      <td>I hate it.</td>\n",
       "      <td>The coversion cable didnt work. t is axlr to U...</td>\n",
       "      <td>Opportunity Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>A1EQYR35KLTECN</td>\n",
       "      <td>B0002E1H9W</td>\n",
       "      <td>animulvr</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>3</td>\n",
       "      <td>1/6/2013</td>\n",
       "      <td>Hate the pushdown bottle heads!</td>\n",
       "      <td>I like Dunlop products but... The push down ap...</td>\n",
       "      <td>Opportunity Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>A1QRF5KISDOKPA</td>\n",
       "      <td>B000SAC5PA</td>\n",
       "      <td>santos</td>\n",
       "      <td>[6, 8]</td>\n",
       "      <td>1</td>\n",
       "      <td>12/23/2011</td>\n",
       "      <td>hate it</td>\n",
       "      <td>ok when i saw the bag i was impressed nice loo...</td>\n",
       "      <td>Opportunity Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>A1VW19Y79DC0GF</td>\n",
       "      <td>B00186L9X2</td>\n",
       "      <td>Andrew M. Ward \"My Stolen Life\"</td>\n",
       "      <td>[13, 16]</td>\n",
       "      <td>3</td>\n",
       "      <td>8/31/2011</td>\n",
       "      <td>It's a Love / Hate Relationship...</td>\n",
       "      <td>It cannot be denied that the BOSS DD-7 is a ve...</td>\n",
       "      <td>Mixed Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>A15BHBF0L0HV1F</td>\n",
       "      <td>B0002D0E8S</td>\n",
       "      <td>Quaestor \"Raoul Duke\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>11/30/2013</td>\n",
       "      <td>Nothing Special</td>\n",
       "      <td>Nothing special. Just like every other strap o...</td>\n",
       "      <td>Mixed Emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>A3DDZ2SENG07MS</td>\n",
       "      <td>B003LTJ404</td>\n",
       "      <td>dashreeve</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>Good stand - not the best for gigging / travel</td>\n",
       "      <td>The stand seems sturdy, the base is like a tri...</td>\n",
       "      <td>Insightful Feedback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>A1EFXXRDV40C4E</td>\n",
       "      <td>B003SZDFM4</td>\n",
       "      <td>Charles Casterline</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>3</td>\n",
       "      <td>12/16/2012</td>\n",
       "      <td>Nothing to brag about</td>\n",
       "      <td>I use to have a guitar that sounded horrible w...</td>\n",
       "      <td>Opportunity Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>A1FCX548TD6DLP</td>\n",
       "      <td>B003QTM9O2</td>\n",
       "      <td>Cooper the Beagle</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>1/9/2014</td>\n",
       "      <td>Poorly Made, Flimsy. Buy Another Product</td>\n",
       "      <td>At the time I bought, was $16. Mine arrived br...</td>\n",
       "      <td>Opportunity Areas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviewerID        asin                     reviewerName   helpful  \\\n",
       "150  A22Z554ZQ8NFPC  B0002D0CKI                       AF \"Whigs\"  [12, 12]   \n",
       "151  A1B9Q3SNKI6T5V  B0002D0LKY                   Dustin Kempton    [0, 0]   \n",
       "152  A3NAA6BH9LWIH4  B001UJEKZ6                    Paul Kacprzak    [0, 2]   \n",
       "153  A1EQYR35KLTECN  B0002E1H9W                         animulvr    [0, 1]   \n",
       "154  A1QRF5KISDOKPA  B000SAC5PA                           santos    [6, 8]   \n",
       "155  A1VW19Y79DC0GF  B00186L9X2  Andrew M. Ward \"My Stolen Life\"  [13, 16]   \n",
       "156  A15BHBF0L0HV1F  B0002D0E8S            Quaestor \"Raoul Duke\"    [0, 0]   \n",
       "157  A3DDZ2SENG07MS  B003LTJ404                        dashreeve    [0, 0]   \n",
       "158  A1EFXXRDV40C4E  B003SZDFM4               Charles Casterline    [2, 2]   \n",
       "159  A1FCX548TD6DLP  B003QTM9O2                Cooper the Beagle    [0, 0]   \n",
       "\n",
       "     overall  reviewTime                                         summary  \\\n",
       "150        3    9/8/2011                                      Good & Bad   \n",
       "151        1    5/2/2014                            I really do hate it.   \n",
       "152        1   5/15/2013                                      I hate it.   \n",
       "153        3    1/6/2013                 Hate the pushdown bottle heads!   \n",
       "154        1  12/23/2011                                         hate it   \n",
       "155        3   8/31/2011              It's a Love / Hate Relationship...   \n",
       "156        3  11/30/2013                                 Nothing Special   \n",
       "157        4    1/6/2012  Good stand - not the best for gigging / travel   \n",
       "158        3  12/16/2012                           Nothing to brag about   \n",
       "159        1    1/9/2014        Poorly Made, Flimsy. Buy Another Product   \n",
       "\n",
       "                                            reviewText  \\\n",
       "150  First, I love these picks and used them exclus...   \n",
       "151  It just randomly pops off my bass, it's so sli...   \n",
       "152  The coversion cable didnt work. t is axlr to U...   \n",
       "153  I like Dunlop products but... The push down ap...   \n",
       "154  ok when i saw the bag i was impressed nice loo...   \n",
       "155  It cannot be denied that the BOSS DD-7 is a ve...   \n",
       "156  Nothing special. Just like every other strap o...   \n",
       "157  The stand seems sturdy, the base is like a tri...   \n",
       "158  I use to have a guitar that sounded horrible w...   \n",
       "159  At the time I bought, was $16. Mine arrived br...   \n",
       "\n",
       "    User Experience Spectrum  \n",
       "150           Mixed Emotions  \n",
       "151        Opportunity Areas  \n",
       "152        Opportunity Areas  \n",
       "153        Opportunity Areas  \n",
       "154        Opportunity Areas  \n",
       "155           Mixed Emotions  \n",
       "156           Mixed Emotions  \n",
       "157      Insightful Feedback  \n",
       "158        Opportunity Areas  \n",
       "159        Opportunity Areas  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccc0820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID                  0.0\n",
       "asin                        0.0\n",
       "reviewerName                0.0\n",
       "helpful                     0.0\n",
       "overall                     0.0\n",
       "reviewTime                  0.0\n",
       "summary                     0.0\n",
       "reviewText                  0.0\n",
       "User Experience Spectrum    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = df.isna().sum() * 100 / df.shape[0]\n",
    "missing_values.sort_values(ascending=False)\n",
    "missing_values\n",
    "# missing_values[missing_values > 0].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481b55b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad11829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical labels for User Experience Spectrum\n",
    "spectrum_labels = {'Thrilled': 0, 'Insightful Feedback': 1, 'Mixed Emotions': 2, 'Opportunity Areas': 3}\n",
    "\n",
    "# Apply labels to the 'user_experience_spectrum' column\n",
    "df['user_experience_labels'] = df['User Experience Spectrum'].map(spectrum_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b33b8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing necessary libraries and tools\n",
    "en_stopwords = set(stopwords.words('english'))  # Set of English stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  # Regular expression tokenizer to split words\n",
    "\n",
    "# Tokenization\n",
    "df['tokenized_text'] = df['reviewText'].apply(tokenizer.tokenize)\n",
    "# Creating a new column 'tokenized_text' containing lists of tokenized words for each review in the 'reviewText' column\n",
    "\n",
    "# Stopword removal\n",
    "stop_words = set(stopwords.words('english'))  # Set of English stopwords\n",
    "df['filtered_tokens'] = df['tokenized_text'].apply(lambda tokens: [word for word in tokens if word.lower() not in en_stopwords])\n",
    "# Creating a new column 'filtered_tokens' containing lists of words after removing stopwords from 'tokenized_text'\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()  # Porter Stemmer for stemming words\n",
    "df['stemmed_tokens'] = df['filtered_tokens'].apply(lambda tokens: [ps.stem(word) for word in tokens])\n",
    "# Creating a new column 'stemmed_tokens' containing lists of stemmed words from 'filtered_tokens'\n",
    "\n",
    "# Combine preprocessed tokens\n",
    "df['processed_text'] = df['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "# Creating a new column 'processed_text' containing preprocessed text by joining the stemmed tokens into a single string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf14b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for 'summary'\n",
    "df['tokenized_summary'] = df['summary'].apply(tokenizer.tokenize)\n",
    "# Creating a new column 'tokenized_summary' containing lists of tokenized words for each summary in the 'summary' column\n",
    "\n",
    "# Stopword removal for 'summary'\n",
    "df['filtered_summary_tokens'] = df['tokenized_summary'].apply(lambda tokens: [word for word in tokens if word.lower() not in en_stopwords])\n",
    "# Creating a new column 'filtered_summary_tokens' containing lists of words after removing stopwords from 'tokenized_summary'\n",
    "\n",
    "# Stemming for 'summary'\n",
    "df['stemmed_summary_tokens'] = df['filtered_summary_tokens'].apply(lambda tokens: [ps.stem(word) for word in tokens])\n",
    "# Creating a new column 'stemmed_summary_tokens' containing lists of stemmed words from 'filtered_summary_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f546353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed tokens for 'summary'\n",
    "df['processed_summary'] = df['stemmed_summary_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "# Creating a new column 'processed_summary' containing preprocessed text by joining the stemmed tokens into a single string for summary\n",
    "\n",
    "# Concatenate 'summary' and 'reviewText' for a holistic input\n",
    "df['combined_text'] = df['processed_text'] + ' ' + df['processed_summary']\n",
    "# Creating a new column 'combined_text' by concatenating the preprocessed text from 'processed_text' and 'processed_summary'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a2083",
   "metadata": {},
   "source": [
    "# Preprocess Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b58e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StratifiedShuffleSplit for splitting\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=30)\n",
    "# Initializing StratifiedShuffleSplit with 1 split, 20% test size, and a random seed for reproducibility\n",
    "\n",
    "# Convert indices to lists\n",
    "train_indices, dev_indices = next(sss.split(df['combined_text'], df['user_experience_labels']))\n",
    "# Applying the split on 'combined_text' and 'user_experience_labels' columns and obtaining the training and development set indices\n",
    "\n",
    "# Split the entire dataset into training and testing\n",
    "X_train, X_test = df['combined_text'].iloc[train_indices].tolist(), df['combined_text'].iloc[dev_indices].tolist()\n",
    "# Extracting the training and testing data for the 'combined_text' column and converting them to lists\n",
    "\n",
    "y_train, y_test = df['user_experience_labels'].iloc[train_indices].tolist(), df['user_experience_labels'].iloc[dev_indices].tolist()\n",
    "# Extracting the corresponding labels for training and testing and converting them to lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea417ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split train_dev into training and development sets\n",
    "train_indices, dev_indices = next(sss.split(X_train, y_train))\n",
    "# Using the previously defined StratifiedShuffleSplit to split the training data (X_train) and its corresponding labels (y_train)\n",
    "# Obtaining indices for training and development sets\n",
    "\n",
    "# Extracting data for training and development sets\n",
    "X_train, X_dev = [X_train[i] for i in train_indices], [X_train[i] for i in dev_indices]\n",
    "# Creating lists of texts for training and development sets based on the indices obtained\n",
    "\n",
    "y_train, y_dev = [y_train[i] for i in train_indices], [y_train[i] for i in dev_indices]\n",
    "# Creating lists of labels for training and development sets based on the indices obtained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4cdd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude Nan values from training\n",
    "df.dropna(subset=['user_experience_labels'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af1d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words (BoW) vectorization\n",
    "bow_vectorizer = CountVectorizer()\n",
    "# Creating a CountVectorizer object for Bag of Words (BoW) vectorization\n",
    "\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "# Transforming the training text data (X_train) into BoW vectors\n",
    "# The fit_transform method both fits the vectorizer to the training data and transforms it\n",
    "\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "# Transforming the testing text data (X_test) into BoW vectors using the same vectorizer\n",
    "# It's important to use the same vectorizer instance for consistency in feature representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63bb0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(X_dev)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74627773",
   "metadata": {},
   "source": [
    "The code below initializes and trains multiple machine learning models, including Support Vector Machine (SVM), Logistic Regression, Naive Bayes, Random Forest, and Gradient Boosting. It then evaluates each model's performance on the development set, printing accuracy and a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af9ad6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Development Accuracy: 0.6154\n",
      "Development Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76        16\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.62        26\n",
      "   macro avg       0.15      0.25      0.19        26\n",
      "weighted avg       0.38      0.62      0.47        26\n",
      "\n",
      "\n",
      "Logistic Regression Development Accuracy: 0.6154\n",
      "Development Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76        16\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.62        26\n",
      "   macro avg       0.15      0.25      0.19        26\n",
      "weighted avg       0.38      0.62      0.47        26\n",
      "\n",
      "\n",
      "Naive Bayes Development Accuracy: 0.6154\n",
      "Development Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76        16\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.62        26\n",
      "   macro avg       0.15      0.25      0.19        26\n",
      "weighted avg       0.38      0.62      0.47        26\n",
      "\n",
      "\n",
      "Random Forest Development Accuracy: 0.6154\n",
      "Development Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76        16\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.62        26\n",
      "   macro avg       0.15      0.25      0.19        26\n",
      "weighted avg       0.38      0.62      0.47        26\n",
      "\n",
      "\n",
      "Gradient Boosting Development Accuracy: 0.5769\n",
      "Development Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74        16\n",
      "           1       0.33      0.20      0.25         5\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.58        26\n",
      "   macro avg       0.24      0.27      0.25        26\n",
      "weighted avg       0.46      0.58      0.50        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train different models\n",
    "models = {\n",
    "    'SVM': SVC(kernel='linear', C=1),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "# Create a dictionary of model names as keys and corresponding model instances as values\n",
    "# Each model is initialized with its specified parameters\n",
    "\n",
    "# Evaluate multiple models\n",
    "for name, model in models.items():\n",
    "    # Iterate through each model in the dictionary\n",
    "    \n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    # Fit the model using the training data (X_train_tfidf) and corresponding labels (y_train)\n",
    "    \n",
    "    # Evaluate on the development set\n",
    "    y_dev_pred = model.predict(X_dev_tfidf)\n",
    "    # Predict the labels for the development set using the trained model\n",
    "    \n",
    "    dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n",
    "    # Calculate the accuracy of the model on the development set\n",
    "    \n",
    "    dev_classification_report = classification_report(y_dev, y_dev_pred)\n",
    "    # Generate a classification report including precision, recall, and F1-score\n",
    "    \n",
    "    # Print the evaluation results for each model\n",
    "    print(f'\\n{name} Development Accuracy: {dev_accuracy:.4f}')\n",
    "    print('Development Classification Report:\\n', dev_classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d304799",
   "metadata": {},
   "source": [
    "## Improving Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f374b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/drey/anaconda3/lib/python3.11/site-packages (2.0.2)\r\n",
      "Requirement already satisfied: numpy in /Users/drey/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\r\n",
      "Requirement already satisfied: scipy in /Users/drey/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Install the XGBoost library if not already installed\n",
    "!pip install xgboost\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Assuming X_train and X_test are lists of texts\n",
    "X_train_array = np.array(X_train)\n",
    "X_test_array = np.array(X_test)\n",
    "\n",
    "# Train a Word2Vec model on the processed text data\n",
    "word2vec_model = Word2Vec(sentences=df['processed_text'], vector_size=100)\n",
    "\n",
    "# Extract vocabulary and vector size from the Word2Vec model\n",
    "vocab = word2vec_model.wv.key_to_index\n",
    "vector_size = 100\n",
    "\n",
    "def get_word2vec_vectors(text):\n",
    "    # Function to obtain the word vectors for each word in a given text\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0 \n",
    "    for word in text.split():\n",
    "        if word in vocab:\n",
    "            index = vocab[word]\n",
    "            vec += word2vec_model.wv[index]  \n",
    "            count += 1\n",
    "    if count > 0:  \n",
    "        vec /= count\n",
    "        return vec\n",
    "\n",
    "# Convert text data to matrices using Word2Vec vectors\n",
    "num_train = X_train_array.shape[0] \n",
    "num_test = X_test_array.shape[0]\n",
    "\n",
    "X_train_w2v_matrix = np.zeros((num_train, vector_size))\n",
    "for i, text in enumerate(X_train_array):\n",
    "    X_train_w2v_matrix[i, :] = get_word2vec_vectors(text)\n",
    "    \n",
    "X_test_w2v_matrix = np.zeros((num_test, vector_size))\n",
    "for i, text in enumerate(X_test_array):\n",
    "    X_test_w2v_matrix[i, :] = get_word2vec_vectors(text)\n",
    "\n",
    "# Train an XGBoost classifier using the Word2Vec matrices\n",
    "# xgb = XGBClassifier()\n",
    "# xgb.fit(X_train_w2v_matrix, y_train)\n",
    "# print(xgb.score(X_test_w2v_matrix, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cdcc7d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a dictionary of hyperparameter values to search over\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(3, 11),  \n",
    "    'learning_rate': np.linspace(.01, 1, 20), \n",
    "    'gamma': np.linspace(.01, 10, 20), \n",
    "    'min_child_weight': np.linspace(.01, 10, 20),  \n",
    "    'subsample': np.linspace(.1, 1, 20),  \n",
    "    'colsample_bytree': np.linspace(.1, 1, 20),  \n",
    "    'reg_alpha': np.linspace(.01, 1, 20), \n",
    "    'reg_lambda': np.linspace(.01, 1, 20),  \n",
    "    'n_estimators': np.arange(10, 100, 5),  \n",
    "    'max_delta_step': np.random.uniform(0.5, 2, 5),  \n",
    "    'colsample_bylevel': np.random.uniform(0.5, 1, 5), \n",
    "    'scale_pos_weight': np.random.uniform(0.5, 2, 5) \n",
    "}\n",
    "\n",
    "# Initialize an XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# The new search strategy\n",
    "# gs_cv = GridSearchCV(xgb, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=5)  # Grid Search\n",
    "rs_cv = RandomizedSearchCV(xgb, param_grid, n_iter=100, scoring=\"accuracy\", n_jobs=-1, cv=5, random_state=42)  # Randomized Search\n",
    "\n",
    "# Perform hyperparameter tuning on the Word2Vec matrix training data\n",
    "rs_cv.fit(X_train_w2v_matrix, y_train)  \n",
    "\n",
    "# Print the best hyperparameters found during the search\n",
    "print(\"Best params:\", rs_cv.best_params_)\n",
    "\n",
    "# Obtain the best model with the optimal hyperparameters\n",
    "best_model = rs_cv.best_estimator_\n",
    "\n",
    "# Predict labels for the test set using the best model\n",
    "y_pred = best_model.predict(X_test_w2v_matrix)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "test_accuracy = best_model.score(X_test_w2v_matrix, y_test)\n",
    "print(\"Test set accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d221510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the best model with the optimal hyperparameters\n",
    "best_model = rs_cv.best_estimator_\n",
    "\n",
    "# Predict labels for the test set using the best model\n",
    "y_pred = best_model.predict(X_test_w2v_matrix)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "test_accuracy = best_model.score(X_test_w2v_matrix, y_test)\n",
    "\n",
    "# Print the test set accuracy\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ed1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have trained your model and made predictions on the development set\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Creating a heatmap for better visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=spectrum_labels.keys(), yticklabels=spectrum_labels.keys())\n",
    "plt.title('Confusion Matrix - Development Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657aefa",
   "metadata": {},
   "source": [
    "### Exporting Model Output for UI implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201497a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(en_stopwords,'stopwords.pkl') \n",
    "joblib.dump(rs_cv,'model.pkl', compress=('zlib', 3))\n",
    "joblib.dump(tfidf_vectorizer,'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a2aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
